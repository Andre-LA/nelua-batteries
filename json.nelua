--[[
The json module.

This module contains utilities to parse chunks JSON texts into
Nelua types and vice-versa.

TODO:
* The function may raise runtime errors when parsing invalid numbers.
* The function may raise runtime errors when parsing integer that overflows.
* JSON emitter.
]]

require 'string'

-- The JSON module.
global json = @record{}

-- Helper to check if `c` is a space.
local function isspace(c: byte): boolean <inline>
  return c == ' '_b or (@cuint)(c)-'\t'_b < 5
end

-- Helper to check if `c` is a hexadecimal digit (matches [0-9a-fA-F]).
local function isxdigit(c: byte): boolean <inline>
  return (@cuint)(c)-'0'_b < 10 or -- [0-9]
         ((@cuint)(c)|32)-'a'_b < 6 -- [a-fA-F]
end

-- Helper to check if `c` is a number digit (matches [A-Fa-f.+-]).
local function isnumdigit(c: byte): boolean <inline>
  return isxdigit(c) or -- hex digit
         c == '+'_b or c == '-'_b or -- sign
         c == '.'_b -- fractional
end

-- Helper to check if `c` is a lowercase character (matches [a-z]).
local function islower(c: byte): boolean <inline>
  return ((@cuint)(c)-'a'_b < 26)
end

-- Helper to convert an integer to its corresponding UTF-8 byte sequence.
local function utf8esc(x: uint32): ([8]byte, int32)
  local buf: [8]byte <noinit>
  local n: int32 = 1
  check(x <= 0x7FFFFFFF_u)
  if x < 0x80 then -- ASCII?
    buf[7] = (@byte)(x)
  else -- need continuation bytes
    local mfb: usize = 0x3f -- maximum that fits in first byte
    repeat -- add continuation bytes
      buf[8 - n] = (@byte)(0x80 | (x & 0x3f))
      n = n + 1
      x = x >> 6 -- removed added bits
      mfb = mfb >> 1 -- now there is one less bit available in first byte
    until x <= mfb -- still needs continuation byte?
    buf[8 - n] = (@byte)((~mfb << 1) | x) -- add first byte
  end
  memory.move(&buf[0], &buf[8-n], n)
  memory.zero(&buf[n], 8-n)
  return buf, n
end

-- Helper to remove escape sequences from a JSON string.
local function unescape(s: string): (string, string)
  local sb: stringbuilder <close>
  local buf: span(byte) = sb:prepare(s.size)
  local i: usize, j: usize
  while i < s.size do
    local c: byte = s.data[i]
    i = i + 1
    if likely(c ~= '\\'_b) then
      buf.data[j] = c
    else
      c = s.data[i]
      i = i + 1
      switch c do
        case '"'_b, '\\'_b, '/'_b then buf.data[j] = c
        case 'b'_b then buf.data[j] = '\b'_b
        case 't'_b then buf.data[j] = '\t'_b
        case 'n'_b then buf.data[j] = '\n'_b
        case 'f'_b then buf.data[j] = '\f'_b
        case 'r'_b then buf.data[j] = '\r'_b
        case 'u'_b then
          for k: usize=i,<i+4 do
            if k >= s.size or not isxdigit(s.data[k]) then
              return (@string){}, 'expected 4 hex digits in utf8 sequence'
            end
          end
          local utf8buf: [8]byte, len: int32 = utf8esc(tointeger((@string){data=&s.data[i],size=4}, 16))
          memory.copy(&buf.data[j], &utf8buf[0], len)
          i = i + 4
          j = j + len - 1
        else return (@string){}, 'unexpected string escape sequence'
      end
    end
    j = j + 1
  end
  sb:commit(j)
  return sb:promote(), (@string){}
end

--[[
Parses JSON text from `chunk` into type `T`.
Returns value of type `T`, plus an error message in case of errors and number of characters parsed.

This parser creates a specialized JSON parser at compile-time,
it's like a JSON parser compiler.

This parser follow these rules:
* `T` is used as a schema and must always be a `record`, `hashmap`, `vector,` or `sequence` type.
* The schema `T` cannot be incomplete or have missing fields, otherwise a parse error will occur.
* The schema `T` types cannot mismatch the JSON chunk, otherwise parse error will occur.
* The schema `T` must always be a `record`, `hashmap`, `vector,` or `sequence` type.
* Missing fields in the JSON chunk will be initialized to zeros.
* Parsed strings and containers allocates new memory.
* Follows JSON 4 spec (JSON 5 is not supported).

The following types are handled:
* `vector` and `sequence`, parsed from JSON arrays.
* `hashmap` where `K` is a `string`, parsed from JSON objects.
* `record` types, parsed from JSON objects.
* `string`, parsed from JSON strings.
* `boolean`, parsed from JSON `true` or `false`.
* All primitive integer and float types are parsed from JSON numbers.
* Records may be nested.
]]
function json.parse(chunk: string, T: type): (auto, string, usize)
  local res: T
  local i: usize = 0
  local s: *[0]byte, slen: usize = chunk.data, chunk.size
  local tmpinit: usize
  local c: byte
  local tmpstr: string, tmpbool: boolean
  -- macros utils
  ## local function skip_whitespaces()
    while i < slen and isspace(s[i]) do i = i + 1 end
  ## end
  ## local function peek_char()
    c = i < slen and s[i] or 0
  ## end
  ## local function skip_comma_peek_char(ec)
    ## peek_char()
    if c ~= ','_b then break end
    i = i + 1
    ## skip_whitespaces() peek_char()
  ## end
  ## local function expect_token(ec, noskip)
    if i >= slen or s[i] ~= #[string.byte(ec)]# then
      return res, #['expected token `'..ec..'`, perhaps due to schema mismatch']#, i
    end
    i = i + 1
    ## if not noskip then skip_whitespaces() end
  ## end
  ## local function expect_string()
    ## expect_token('"', true)
    tmpinit = i
    while i < slen and (s[i] ~= '"'_b or s[i-1] == '\\'_b) do i = i + 1 end
    tmpstr = string{data=&s[tmpinit], size=i-tmpinit}
    ## expect_token'"'
  ## end
  ## local function expect_number()
    tmpinit = i
    while i < slen and isnumdigit(s[i]) do i = i + 1 end
    tmpstr = string{data=&s[tmpinit], size=i-tmpinit}
    ## skip_whitespaces()
  ## end
  ## local function expect_boolean()
    tmpinit = i
    while i < slen and islower(s[i]) do i = i + 1 end
    tmpstr = string{data=&s[tmpinit], size=i-tmpinit}
    ## skip_whitespaces()
  ## end
  ## local function expect_value(vtype)
    ## if vtype.is_float then
      ## expect_number()
      local v: #[vtype]# = tonumber(tmpstr)
    ## elseif vtype.is_integral then
      ## expect_number()
      local v: #[vtype]# = tointeger(tmpstr)
    ## elseif vtype.is_boolean then
      ## expect_boolean()
      local v: #[vtype]# = tmpstr == 'true'
      if not v and tmpstr ~= 'false' then
        return res, 'expected a boolean, perhaps due to schema mismatch', i
      end
    ## elseif vtype.is_string then
      ## expect_string()
      local v: #[vtype]#, err: string = unescape(tmpstr)
      if err.size > 0 then return res, err, i end
    ## elseif vtype.is_record then
      local v: #[vtype]#, err: string, advlen: usize = json.parse(string{data=&s[i],size=slen-i}, #[vtype]#)
      i = i + advlen
      if err.size > 0 then return res, err, i end
    ## else static_error("cannot parse element of type '%s'", vtype) end
  ## end
  -- begin parsing
  ## skip_whitespaces() -- skip leading whitespaces
  ## if res.type.is_vector or res.type.is_sequence then -- parse array into vector/sequence
    ## expect_token'[' peek_char()
    while c ~= ']'_byte do
      ## expect_value(res.type.subtype)
      res:push(v)
      ## skip_comma_peek_char()
    end
    ## expect_token']'
  ## elseif res.type.is_hashmap then -- parse object into hashmap
    local fieldname: string <noinit>
    ## expect_token'{' peek_char()
    while c ~= '}'_byte do
      ## expect_string()
      fieldname = tmpstr
      ## expect_token':' expect_value(res.type.V)
      res[fieldname] = v
      ## skip_comma_peek_char()
    end
    ## expect_token'}'
  ## elseif res.type.is_record then -- parse object into a record
    local fieldname: string <noinit>
    ## expect_token'{' peek_char()
    while c ~= '}'_byte do
      ## expect_string()
      fieldname = tmpstr
      ## expect_token':'
      ## for _,field in ipairs(res.type.fields) do
        if fieldname == #[field.name]# then
          ## expect_value(field.type)
          res.#|field.name|# = v
          goto nextfield
        end
      ## end
      return res, "found extra fields not defined in the schema", i
::nextfield::
      ## skip_comma_peek_char()
    end
    ## expect_token'}'
  ## else static_error("cannot parse JSON into type '%s'", T) end
  return res, (@string){}, i
end
